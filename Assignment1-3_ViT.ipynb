{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning Assignment #1<br> Part 1-3. Training Vision Transformers (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jaehoon Lee, September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PDF file.**\n",
    "\n",
    "Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **PyTorch**. <br>\n",
    "In this notebook, you will learn to understand and build the basic components of Vision Tranformer(ViT). Then, you will try to classify images in the FashionMNIST datatset and explore the effects of different components of ViTs.\n",
    "<br>\n",
    "There are **2 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results. \n",
    "\n",
    "### Some helpful tutorials and references for assignment #1-2:\n",
    "- [1] Pytorch official documentation. [[link]](https://pytorch.org/docs/stable/index.html)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Alexey Dosovitskiy et al., \"An Image is Worth 16 x 16 Words: Transformers for Image Recognition at Scale\", ICLR 2021. [[pdf]](https://arxiv.org/pdf/2010.11929.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Vision Transformer\n",
    "Here, you will build the basic components of Vision Transformer(ViT). <br>\n",
    "\n",
    "![Vision Transformer](imgs/ViT.png)\n",
    "\n",
    "Using the explanation and code provided as guidance, <br>\n",
    "Define each component of ViT. <br>\n",
    "\n",
    "\n",
    "#### ViT architecture:\n",
    "* ViT model consists with input patch embedding, positional embeddings, transformer encoder, etc.\n",
    "* Patch embedding\n",
    "* Positional embeddings\n",
    "* Transformer encoder with\n",
    "    * Attention module\n",
    "    * MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Patch Embed\n",
    "\n",
    "**Initialization**: When you create an instance of the PatchEmbedding class, you specify the image_size, patch_size, and in_channels. image_size is the height and width of the input image, patch_size is the size of each patch, and in_channels is the number of input image channels (e.g., 3 for RGB images). \n",
    "\n",
    "**Convolutional Projection**: Inside the PatchEmbedding class, a 2D convolutional layer (nn.Conv2d) is used to perform a patch-based projection. This convolutional layer has a kernel size of patch_size, which defines the size of each patch, and a stride of patch_size, which ensures that patches do not overlap. The convolutional layer effectively extracts image patches.\n",
    "\n",
    "**Reshaping**: After the convolutional projection, the output tensor is reshaped using view. It is transformed from a 4D tensor with dimensions (batch_size, in_channels, H, W) to a 3D tensor with dimensions (batch_size, num_patches, patch_dim). num_patches is the total number of non-overlapping patches in the image, and patch_dim is the number of output channels from the convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x # output dimension must be: (batch size, number of patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention\n",
    "\n",
    "**Initialization**\n",
    "* dim: The input dimension of the sequence. This is the dimensionality of the queries, keys, and values.\n",
    "* num_heads: The number of attention heads to use. Multi-head attention allows the model to focus on different parts of the input simultaneously.\n",
    "\n",
    "**Linear Projections (qkv and proj)**: The qkv linear layer takes the input sequence and projects it into three parts: queries (q), keys (k), and values (v). The output of this layer has a shape of (batch_size, sequence_length, 3 * dim).\n",
    "\n",
    "**Forward Pass (forward method)**: In the forward pass, the input tensor x is processed through the attention mechanism. Here's what happens:<br>\n",
    "* The linear projection qkv is applied to x, producing a tensor of shape (batch_size, sequence_length, 3 * dim).|\n",
    "* This tensor is reshaped to have dimensions (batch_size, sequence_length, 3, num_heads, head_dim). The permute operation rearranges the dimensions to (3, batch_size, num_heads, sequence_length, head_dim), making it suitable for multi-head attention.\n",
    "* The three parts, q, k, and v, are extracted from the reshaped tensor.\n",
    "* The attention scores are computed by taking the dot product of queries q and keys k. The result is scaled by self.scale.\n",
    "* The attention scores are passed through a softmax activation along the last dimension (sequence_length), producing attention weights.\n",
    "* The weighted sum of values v is computed using the attention weights.\n",
    "* The result is transposed and reshaped to its original shape, and then passed through the proj linear layer.\n",
    "* The final output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x # output dimension must be: (batch size, number of patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP\n",
    "\n",
    "The MLP module must consist of three layers:\n",
    "* fully conncted layer 1\n",
    "* activation layer\n",
    "* fully conncted layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x # output dimension must be: (batch size, number of patches, out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer Block\n",
    "The transformer block contains the attention module and MLP module which have residual connections. \n",
    "Refer to the following image and build the forward pass.\n",
    "\n",
    "![Transformer Block](imgs/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vision Transformer\n",
    "\n",
    "Using all the components that you built above, **complete** the vision transformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=28, patch_size=4, in_chans=1, num_classes=10, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., norm_layer=nn.LayerNorm, ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ############################################################################## \n",
    "        # similarly to cls_token, define a learnable positional embedding that matches the patchified input token size.\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,  norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Concatenate class tokens to patch embedding\n",
    "\n",
    "        # Add positional embedding to patches\n",
    "\n",
    "        # Forward through encoder blocks\n",
    "\n",
    "        # Use class token for classification\n",
    "        \n",
    "        # Classifier head\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a small ViT model on FashionMNIST dataset.\n",
    "\n",
    "Define and Train a vision transformer on FashionMNIST dataset. **(You must reach above 85% for full points.)** <br>\n",
    "Train with at least 5 different hyperparameter settings varying the following ViT hyperparameters. \n",
    "Report the setting for the best performance.\n",
    "\n",
    "#### ViT hyperparameters:\n",
    "* patch_size\n",
    "* embed_dim\n",
    "* depth\n",
    "* num_heads\n",
    "* mlp_ratio\n",
    "* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train():\n",
    "    ##############################################################################\n",
    "    #                           IMPLEMENT YOUR CODE                              #\n",
    "    ##############################################################################\n",
    "\n",
    "    patch_size=\n",
    "    embed_dim=\n",
    "    depth=\n",
    "    num_heads= # make sure embed_dim is divisible by num_heads!\n",
    "    mlp_ratio=\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                              END YOUR CODE                                 #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # Loading data\n",
    "    transform = ToTensor()\n",
    "\n",
    "    train_set = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_set = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n",
    "    # Defining model and training options\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    \n",
    "    model = VisionTransformer(patch_size=patch_size, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio).to(device)\n",
    "    model_path = './vit.pth'\n",
    "    N_EPOCHS = 5\n",
    "    LR = 0.005\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('Saved Trained Model.')\n",
    "    \n",
    "Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did and discovered here\n",
    "In this cell you should write all the settings tried and performances you obtained. Report what you did and what you discovered from the trials.\n",
    "You can write in Korean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tell us here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
