{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning Assignment #1<br> Part 1-4. Finetune BERT (Pytorch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by JunYong Ahn, September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PDF file.**\n",
    "\n",
    "In this notebook, you will learn how to fine-tune the lightweight BERT variant using Hugging Face's transformers library for text classification tasks; BERT(Bidirectional Encoder Representations from Transformers) is a groundbreaking model in the NLP domain.  <br>\n",
    "There are **2 sections**, and in each section, you need to follow the instructions to complete the skeleton codes.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.\n",
    "\n",
    "### Some helpful tutorials and references for assignment #1-4:\n",
    "- [1] BERT original paper (Devlin et al., 2018). [[link]](https://arxiv.org/abs/1810.04805)\n",
    "- [2] Tutorials about BERT [[link]](https://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check virtual env and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ[\"CONDA_DEFAULT_ENV\"] == \"deep-learning-23\", \"current environment is not deep-learning-23\"\n",
    "!python3 -m pip install pandas\n",
    "!python3 -m pip install transformers\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "if torch.cuda.is_available() is True:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Finetune BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "1. Download the dataset from attached link.\n",
    "2. Move the downloaded zip file under the \"data\" directory and then unzip the zip file.\n",
    "3. Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_file_path):\n",
    "    if os.path.exists(data_file_path):\n",
    "        df = pd.read_csv(data_file_path)\n",
    "        texts = df['review'].tolist()\n",
    "        labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
    "        return texts, labels\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The file '{data_file_path}' does not exist.\")\n",
    "\n",
    "data_file_path = './data/IMDB Dataset.csv'\n",
    "texts, labels = load_imdb_data(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_seq_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classifier head for BERT (Design your model's prediction head )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(CustomBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        ######################## TO-DO ########################\n",
    "\n",
    "        \n",
    "        \n",
    "        ######################## TO-DO ########################\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        ######################## TO-DO ########################\n",
    "\n",
    "        \n",
    "        \n",
    "        ######################## TO-DO ########################\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in tqdm(data_loader, desc=\"Train\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "# Hint: generally, less than 10 epochs will be enough.\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "######################## TO-DO ########################\n",
    "max_seq_length = \n",
    "batch_size = \n",
    "num_epochs = \n",
    "learning_rate = \n",
    "######################## TO-DO ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DO NOT CHANGE ########################\n",
    "train_texts, val_texts, train_labels, val_labels = \\\n",
    "train_test_split(texts, labels, test_size=0.4, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = \\\n",
    "train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
    "######################## DO NOT CHANGE ########################\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = CustomTextClassificationDataset(train_texts, train_labels, tokenizer, max_seq_length)\n",
    "val_dataset = CustomTextClassificationDataset(val_texts, val_labels, tokenizer, max_seq_length)\n",
    "test_dataset = CustomTextClassificationDataset(test_texts, test_labels, tokenizer, max_seq_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, optimizer, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CustomBERTClassifier(bert_model_name, num_classes).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_model(model, train_dataloader, optimizer, scheduler, device)\n",
    "    accuracy, report = evaluate_model(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "# evaluation\n",
    "accuracy, report = evaluate_model(model, test_dataloader, device)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "696448c147db95129aac2fd56dc49d38b08c0b6499dfa51590df1ba99082e5b1"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
