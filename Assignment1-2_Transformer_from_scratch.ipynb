{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning Assignment #1<br> Part 1-2. Transformer from scratch (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by JunYong Ahn, September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully \n",
    "look at given PDF file.**\n",
    "\n",
    "In this notebook, you will learn to implement a transformer model from scratch. By doing so, you will understand the nuts and bolts of Transformers more clearly at a code level.\n",
    "<br>\n",
    "There are **5 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "\n",
    "### Some helpful tutorials and references for assignment #1-3:\n",
    "- [1] Original Transformer paper(Vaswani et al., 2017). [[link]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n",
    "- [2] Helpful instructions about how Transformer works. [[link]](https://github.com/jadore801120/attention-is-all-you-need-pytorch)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check virtual env and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ[\"CONDA_DEFAULT_ENV\"] == \"deep-learning-23\", \"current environment is not deep-learning-23\"\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() is True:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoder](./imgs/Model_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original paper on Transformer, positional encoding is constructed by using sine functions to even dimensions and cosine functions to odd dimensions.\n",
    "\n",
    "\\begin{align*}\n",
    "    PE_{(pos,2i)} = sin(pos / 10000^{2i/dim}) \\\\\n",
    "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/dim})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, seq_len_max):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        PE = torch.zeros(seq_len_max, dim)\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        ######################### DO NOT CHANGE #########################\n",
    "        # Positional Encoding is not learnable parameters.\n",
    "        self.register_buffer('PE', PE.unsqueeze(0))\n",
    "        ######################### DO NOT CHANGE #########################\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X + self.PE[:, :X.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multi_head_attention](./imgs/Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement MultiHeadAttention Class.  \n",
    "The parameters of MultiHeadAttention class is defined as follows. \n",
    "Note that according to the definition of multi-head attention, the dimension of the model is equal to the product\n",
    "of the word dimension and the number of heads\n",
    "\n",
    "$dim$:  dimension of the model  \n",
    "$dim$ = dimension for a each word * $head\\_num$  \n",
    "$seq\\_len$:  length of the input sequence\n",
    "\n",
    "This module will get batched sequences x and return multi-head attention ouput. \n",
    "\n",
    "X size:  $(batch\\_num, seq\\_len, dim)$  \n",
    "mask: Tensor to indicate the words involved in score calculation  \n",
    "output size:  $(batch\\_num, seq\\_len, dim)$\n",
    "\n",
    "$W_q$ = linear transformation for query  \n",
    "$W_k$ = linear transformation for key    \n",
    "$W_v$ = linear transformation for value  \n",
    "$W_o$ = linear transformation for concatenated heads\n",
    "\n",
    "The model operates according to the following equation.  \n",
    "It should select the values that will participate in score calculation based on the received mask.\n",
    "\n",
    "$Q = X * W_q$  \n",
    "$K = X * W_k$  \n",
    "$V = X * W_v$  \n",
    "\n",
    "$scores = \\frac{QK^T}{\\sqrt{word\\_dim}}$  \n",
    "$masked\\_scores = mask(\\frac{QK^T}{\\sqrt{word\\_dim}})$  \n",
    "$probs = softmax(masked\\_scores)$  \n",
    "$heads = probsV$  \n",
    "$output = heads * W_o$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, head_num):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.head_num = head_num\n",
    "        self.word_dim = dim // head_num\n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        self.W_q = \n",
    "        self.W_k = \n",
    "        self.W_v = \n",
    "        self.W_o = \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        return heads\n",
    "    \n",
    "    def split(self, X):\n",
    "        batch_num, seq_len, dim = X.size()\n",
    "        return X.view(batch_num, seq_len, self.head_num, self.word_dim).transpose(1, 2)\n",
    "        \n",
    "    def combine(self, X):\n",
    "        batch_num, _, seq_len, _ = X.size()\n",
    "        return X.transpose(1, 2).contiguous().view(batch_num, seq_len, self.dim)\n",
    "        \n",
    "    def forward(self, X_Q, X_K, X_V, mask=None):\n",
    "        Q = self.split(self.W_q(X_Q))\n",
    "        K = self.split(self.W_k(X_K))\n",
    "        V = self.split(self.W_v(X_V))\n",
    "        \n",
    "        heads = self.scaled_dot_product(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine(heads))\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement EncoderLayer class using **one MultiHeadAttention layer, one FNN layer and two normalization layer**.  \n",
    "**Please apply dropout right after passing through multi-head attention and FFN layer.**\n",
    "\n",
    "**HINT**  \n",
    "**1. Normalization is a LayerNorm.**  \n",
    "**2. LayerNorm layers have learnable parameters. Therefore, you should use two normalization layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, FFN_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.FFN_layer = nn.Sequential(nn.Linear(dim, FFN_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(FFN_dim, dim))\n",
    "    def forward(self, X):\n",
    "        return self.FFN_layer(X)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, FFN_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "    def forward(self, X, mask):\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement DecoderLayer class using **two MultiHeadAttention layers(self-attention and cross-attention), one FNN layer and three normalization layers.** \n",
    "**Please apply dropout right after passing through two multi-head attention layers and FFN layer.**\n",
    "\n",
    "**HINT**  \n",
    "**1. Normalization is a LayerNorm.**  \n",
    "**2. LayerNorm layers have learnable parameters. Therefore, you should use three normalization layers.**  \n",
    "**3. The first multi-head attention layer is a self attention layer, and the second attention layer is a cross attention layer. Choose the mask carefully.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, FFN_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "    def forward(self, X, enc_output, cross_attn_mask, self_attn_mask):\n",
    "        ######################### TO DO #########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### TO DO #########################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare sample data and Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_lib_size, output_lib_size, dim, head_num, layer_num, \\\n",
    "                 FFN_dim, seq_len_max, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.enc_embeds = nn.Embedding(input_lib_size, dim)\n",
    "        self.dec_embeds = nn.Embedding(output_lib_size, dim)\n",
    "        self.pe = PositionalEncoding(dim, seq_len_max)\n",
    "\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(dim, head_num, FFN_dim, dropout) \\\n",
    "                                             for _ in range(layer_num)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(dim, head_num, FFN_dim, dropout) \\\n",
    "                                             for _ in range(layer_num)])\n",
    "        self.Linear = nn.Linear(dim, output_lib_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        self_attn_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        cross_attn_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        nopeak_mask = nopeak_mask.to(device)\n",
    "        cross_attn_mask = cross_attn_mask & nopeak_mask\n",
    "        return self_attn_mask, cross_attn_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self_attn_mask, cross_attn_mask = self.generate_mask(src, tgt)\n",
    "        src_embeds = self.dropout(self.pe(self.enc_embeds(src)))\n",
    "        tgt_embeds = self.dropout(self.pe(self.dec_embeds(tgt)))\n",
    "\n",
    "        enc_output = src_embeds\n",
    "        for enc_layer in self.encoder:\n",
    "            enc_output = enc_layer(enc_output, self_attn_mask)\n",
    "\n",
    "        dec_output = tgt_embeds\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_output = dec_layer(dec_output, enc_output, self_attn_mask, cross_attn_mask)\n",
    "\n",
    "        output = self.Linear(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lib_size = 5000\n",
    "output_lib_size = 5000\n",
    "dim = 512\n",
    "head_num = 4\n",
    "layer_num = 3\n",
    "FFN_dim = 2048\n",
    "seq_len_max = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(input_lib_size, output_lib_size, dim, head_num, layer_num, \\\n",
    "                          FFN_dim, seq_len_max, dropout)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, input_lib_size, (64, seq_len_max)).to(device)  \n",
    "tgt_data = torch.randint(1, output_lib_size, (64, seq_len_max)).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, output_lib_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
